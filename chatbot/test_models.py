# File: chatbot/test_models.py

import os
import json
import gc
import time
import logging
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from chatbot.config import MODEL_CONFIGS, CACHE_DIR
from chatbot.model import MODEL_PATHS

# Configuraci√≥n de logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
logger = logging.getLogger(__name__)

SAVE_DIR = "chatbot/saved_chats"

INTERNAL_TO_FRIEND = { internal: friendly for friendly, internal in MODEL_PATHS.items()}

# 1) Definimos un SYSTEM_PROMPT general, y opcionalmente por modelo
DEFAULT_SYSTEM_PROMPT = (
    "Eres un asistente que siempre es √∫til y ayuda con todo lo que sabe, "
    "pero a menos que te pidan ayuda con idiomas solo podr√°s responder en ESPA√ëOL."
)

PER_MODEL_PROMPT = {
    "Llama-3.2-3B-Instruct": (
        "Eres un experto en IA con un amplio conocimiento de los conceptos de IA d√©bil y fuerte. "
        "Proporciona respuestas detalladas, usa ejemplos y clarifica por qu√© cada t√©rmino recibe ese nombre."
    ),
    "Instella-3B-Instruct": (
        "Eres un asistente optimizado por AMD, excelente explicando procesos y cadenas de razonamiento "
        "de forma clara y fluida. Aprovecha tu fuerza en coherencia para estructurar bien las respuestas."
    ),
    "Qwen2.5-3B-Instruct": (
        "Eres un especialista en respuestas estructuradas. Ofrece esquemas, listas y subt√≠tulos cuando sea √∫til."
    ),
    "Stable-Code-Instruct-3B": (
        "Eres un asistente de programaci√≥n: explica paso a paso la correcci√≥n de errores y genera c√≥digo limpio."
    ),
}

TEST_QUESTIONS = {
    "Llama-3.2-3B-Instruct": [
        "¬øQu√© diferencia hay entre la inteligencia artificial d√©bil y fuerte?",
        "Resume en una frase este texto: La inteligencia artificial es una disciplina que busca crear sistemas que puedan realizar tareas que requieren inteligencia humana.",
        "¬øC√≥mo se dice ‚ÄúNo tengo tiempo para esto‚Äù en japon√©s?",
        "¬øCu√°l es la capital de Mongolia y cu√°ntos habitantes tiene?",
        "¬øPodr√≠as explicarlo como si tuviese 10 a√±os?"
    ],
    "Instella-3B-Instruct": [
        "¬øQu√© pasos seguir√≠as para preparar un sistema de recomendaci√≥n?",
        "Si A implica B y B implica C, ¬øqu√© se puede concluir sobre A y C?",
        "¬øQu√© errores ves en este pseudoc√≥digo?\n\nif user = \"admin\":\n    print(\"Access granted\")\nelse:\n    print(\"Denied\")",
        "Redacta un correo profesional para pedir presupuesto a una empresa de servicios IT.",
        "Explica c√≥mo resolver√≠as un Sudoku paso a paso."
    ],
    "Qwen2.5-3B-Instruct": [
        "¬øCu√°les son las ventajas y desventajas del teletrabajo?",
        "Dime c√≥mo organizar un viaje a Jap√≥n en 5 pasos.",
        "¬øPuedes explicarme el ciclo del agua usando subt√≠tulos para cada fase?",
        "Hazme un esquema de los tipos de inteligencia seg√∫n Howard Gardner.",
        "¬øQu√© necesito para montar un servidor web casero?"
    ],
    "Stable-Code-Instruct-3B": [
        "Corrige este error en Python: TypeError: unsupported operand type(s) for +: 'int' and 'str'",
        "¬øC√≥mo har√≠as una funci√≥n en JavaScript para detectar si un n√∫mero es primo?",
        "¬øQu√© hace este fragmento de c√≥digo?\n\ndef f(x): return x if x == 0 else f(x - 1) + 1",
        "Gen√©rame un script Bash para hacer backup de una carpeta cada d√≠a.",
        "Escribe una clase en Python con __init__, __str__ y un m√©todo update()."
    ]
}

def run_tests_and_save():
    os.makedirs(SAVE_DIR, exist_ok=True)
    logger.info(f"‚û°Ô∏è Directorio de ejemplos: '{SAVE_DIR}'\n")

    for alias, cfg in MODEL_CONFIGS.items():
        repo_id = cfg["repo_id"]
        example_path = os.path.join(SAVE_DIR, f"Ejemplo_{alias}.json")
        if os.path.exists(example_path):
            logger.info(f"‚úÖ Ejemplo ya existe para ‚Äò{alias}‚Äô, lo salto.\n")
            continue

        logger.info(f"üß† Iniciando pruebas para modelo: {alias}")
        cache_dir = os.path.join(CACHE_DIR, alias)
        logger.info(f"  üìÅ Cache dir: {cache_dir}")

        # Preparar kwargs de generaci√≥n
        gen_kwargs = cfg["pipeline_kwargs"].copy()
        gen_kwargs.pop("early_stopping", None)
        gen_kwargs.pop("no_repeat_ngram_size", None)
        gen_kwargs["do_sample"] = True
        logger.info(f"  ‚öôÔ∏è Pipeline kwargs: {gen_kwargs}")

        # trust_remote_code si es necesario
        trust_remote = cfg.get("trust_remote_code", False) or "Instella" in alias
        if trust_remote:
            logger.warning("  ‚ö†Ô∏è trust_remote_code=True")

        # Calcular auto-asignaci√≥n de memoria GPU (90%)
        if torch.cuda.is_available():
            props = torch.cuda.get_device_properties(0)
            total_mib = props.total_memory // (1024**2)
            allow_mib = int(total_mib * 0.9)
            max_mem_arg = {0: f"{allow_mib}MiB"}
            logger.info(f"    üñ•Ô∏è Asignando hasta {allow_mib}MiB en device_map")
        else:
            max_mem_arg = None

        # Cargar tokenizer y modelo
        start = time.time()
        tokenizer = AutoTokenizer.from_pretrained(
            repo_id,
            cache_dir=cache_dir,
            trust_remote_code=trust_remote
        )
        model = AutoModelForCausalLM.from_pretrained(
            repo_id,
            cache_dir=cache_dir,
            trust_remote_code=trust_remote,
            device_map="auto",
            torch_dtype=torch.float16,
            **({"max_memory": max_mem_arg} if max_mem_arg else {})
        )
        elapsed = time.time() - start
        logger.info(f"  üì¶ Modelo cargado en {elapsed:.1f}s con device_map='auto'")

        # Construir pipeline
        pipe = pipeline(
            task=cfg["task"],
            model=model,
            tokenizer=tokenizer,
            **gen_kwargs
        )

        # Iterar preguntas
        history = []
        for i, question in enumerate(TEST_QUESTIONS[alias], start=1):
            # 2) Preparamos el prompt completo:
            system_prompt = PER_MODEL_PROMPT.get(alias, DEFAULT_SYSTEM_PROMPT)
            full_input = f"{system_prompt}\n\nUsuario: {question}\nAsistente:"
            
            logger.info(f"  üìù Pregunta {i}: {question!r}")
            q_start = time.time()
            try:
                out = pipe(full_input, return_full_text=False)[0]["generated_text"]
                # opcionalmente, si el modelo repite todo el prompt:
                if "Usuario:" in out:
                    out = out.split("Usuario:")[0].rstrip()
                dur = time.time() - q_start
                snippet = out.replace("\n", " ")[:200]
                logger.info(f"    üí¨ Respuesta ({dur:.1f}s): {snippet}{'...' if len(out)>200 else ''}")
            except Exception as e:
                dur = time.time() - q_start
                logger.error(f"    ‚ùå Error tras {dur:.1f}s: {e}")
                out = f"[ERROR] {e}"

            history.append({"role": "user",      "content": question})
            history.append({"role": "assistant", "content": out})

            # Mostrar uso de GPU tras cada respuesta
            if torch.cuda.is_available():
                alloc = torch.cuda.memory_allocated()  / 1024**2
                resv  = torch.cuda.memory_reserved()   / 1024**2
                logger.info(f"    üñ•Ô∏è GPU alloc: {alloc:.1f}MiB, reserved: {resv:.1f}MiB")

        # Guardar historial de ejemplo
        friendly_name = INTERNAL_TO_FRIEND.get(alias, alias)
        with open(example_path, "w", encoding="utf-8") as f:
            json.dump({"model": friendly_name, "history": history}, f, ensure_ascii=False, indent=2)
        logger.info(f"‚úÖ Guardado como '{example_path}'")

        # Liberar memoria
        del pipe, model, tokenizer
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        logger.info(f"üîª Memoria vaciada tras prueba de ‚Äò{alias}‚Äô\n")

if __name__ == "__main__":
    run_tests_and_save()
