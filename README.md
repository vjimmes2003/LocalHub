# LocalHub

![Python](https://img.shields.io/badge/python-%3E%3D3.10-blue) ![MIT](https://img.shields.io/badge/license-MIT-green) ![GPU-ready](https://img.shields.io/badge/GPU-Compatible-green)

**LocalHub** es una aplicaciÃ³n **local** (no servidor pÃºblico) basada en **FastAPI + Gradio** que integra tres mini-apps de inteligencia artificial:

- ğŸ¤– **Chatbot Multi-Modelo** (~3 B parÃ¡metros)  
- ğŸ–¼ï¸ **Generador de imÃ¡genes** con Realistic Vision V6 B1 y Juggernaut XL v9
- ğŸ¤ **Transcriptor de audio** con Whisper (â€œlightâ€ y â€œturboâ€)

# Preview:

### ğŸ–¼ï¸ Vista general de LocalHub

![Vista general](images/readme/landing.png)

---

### ğŸ¤– Chatbot Multi-Modelo

![Chatbot en uso](images/readme/chatbot_demo.png)

---

### ğŸ–¼ï¸ Generador de ImÃ¡genes

![Generador IA](images/readme/images_demo.png)

---

### ğŸ¤ Transcriptor de audio

![Transcriptor de audio](images/readme/spch_to_text_demo.png)

---

## ğŸ“‘ Ãndice

1. [ğŸ“‹Requisitos](#1-requisitos)  
2. [ğŸ”§InstalaciÃ³n](#2-instalaciÃ³n)  
3. [ğŸš€Despliegue WebApp](#3-despliegue-de-la-aplicaciÃ³n-web)  
4. [ğŸ¤–Chatbot Multi-Modelo Personalizable](#4-chatbot-multi-modelo-personalizable)  
5. [ğŸ–¼ï¸Generador de ImÃ¡genes](#5-generador-de-imÃ¡genes)  
6. [ğŸ¤Transcriptor de Audio (Whisper)](#6-transcriptor-de-audio-whisper)
7. [âœ¨Extras](#7âœ¨-extras)   
8. [ğŸ“„Licencia](#8ğŸ“„-licencia)


---

## 1. ğŸ“‹ Requisitos

- **Python** â‰¥ 3.10  
- **CUDA** 12.x + driver compatible _(opcional para GPU)_
- **SO**: Windows / Linux / macOS  
- **RAM**: â‰¥ 8 GB  
- **VRAM**:  
  - â‰¥ 4 GB (minimo) 
  - â‰¥ 6 GB (bÃ¡sico) 
  - â‰¥ 8 GB (recomendado)
  - â‰¥ 10 GB (ideal)   

---

## 2. ğŸ”§ InstalaciÃ³n

### 2.1) Clonar el repositorio

```bash
git clone https://github.com/vjimmes2003/LocalHub.git
cd LocalHub
```

### 2.2) Crear el entorno virtual

```bash
python -m venv .venv
```

### 2.3) Activar el entorno virtual

**PowerShell:**
```bash
.venv\Scripts\Activate.ps1
```

**CMD:**
```bash
.venv\Scripts\activate.bat
```

### 2.4) Actualizar pip e instalar dependencias

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 2.5) Iniciar sesiÃ³n en Hugging Face CLI

```bash
huggingface-cli login
```

Visita [https://huggingface.co/settings/token](https://huggingface.co/settings/token) y genera un **token de lectura**.  

â„¹ï¸ **Nota**: El modelo **Llama 3.2** es *gated* y requiere aprobaciÃ³n previa.  
Puedes solicitar acceso desde [la pÃ¡gina oficial del modelo](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct).

Pega el token con clic derecho cuando te lo pida la consola.


### 2.6) ğŸ§° Hotfix de `basicsr` (hasta que se  arregle)

`RealESRGAN` depende de la librerÃ­a `basicsr`, que puede tener un **bug en versiones recientes** (import desde `functional_tensor` en lugar de `functional`).

Para evitar errores, una vez has terminado todo ejecuta este script:

```bash
python hotfix_basicsr.py
```

### 2.7) â—Instalar `ffmpeg` (requerido para audio)

Whisper necesita `ffmpeg` para procesar correctamente los archivos `.mp3` y `.wav`.

#### Windows:
Usa Chocolatey (recomendado):

```bash
choco install ffmpeg
```
---

## 3.ğŸš€ Despliegue de la aplicaciÃ³n web

Ejecuta todas las mini-apps con un solo comando:

```bash
uvicorn app:app --host 0.0.0.0 --port 5123
```

- Los modelos se descargan y cargan en `chatbot/models/â€¦` la primera vez que los usas.  
- Detecta y usa GPU si estÃ¡ disponible (si no, cae a CPU sin problemas).  
- La interfaz de Gradio agrupa las tres funcionalidades:

  - ğŸ¤– **Chatbot Multi-Modelo**  
  - ğŸ–¼ï¸ **Generador de ImÃ¡genes**  
  - ğŸ¤ **Transcriptor de Audio (Whisper)**

---

## 4.ğŸ¤– Chatbot Multi-Modelo Personalizable

InteractÃºa con cuatro modelos de ~3 B parÃ¡metros. Cada uno estÃ¡ optimizado para una tarea especÃ­fica:

| Selector        | Alias interno               | Especialidad                                          |
|-----------------|-----------------------------|-------------------------------------------------------|
| **Llama 3.2**   | Llama-3.2-3B-Instruct       | Tareas generales y explicaciones multilingÃ¼es        |
| **Instella 3B** | Instella-3B-Instruct        | Razonamiento paso a paso y conversaciÃ³n fluida       |
| **Qwen2.5**     | Qwen2.5-3B-Instruct         | Esquemas, listas y respuestas bien estructuradas     |
| **Stable-Code** | stable-code-instruct-3b     | CÃ³digo, explicaciones tÃ©cnicas y depuraciÃ³n          |

---

### 4.1âš™ï¸ CaracterÃ­sticas

#### 4.1.1ğŸ“¦ Carga y cachÃ© local

- Primer uso descarga el modelo en `chatbot/models/<Alias>/`
- Usa `cache_dir` para evitar descargas repetidas

#### 4.1.2âš–ï¸ GestiÃ³n de GPU/CPU

- `device_map="auto"` reparte el modelo entre GPU y CPU  
- `torch_dtype=float16` reduce el uso de VRAM  
- Reserva el 90% de la GPU para el modelo (el resto se deja como buffer)  
- Limpia la cachÃ© al cambiar de modelo con `torch.cuda.empty_cache()`

#### 4.1.3ğŸ§¾ Prompts inteligentes

- `DEFAULT_SYSTEM_PROMPT`: obliga a responder siempre en **espaÃ±ol**
- `PER_MODEL_PROMPT`: adapta las respuestas segÃºn la especializaciÃ³n del modelo

#### 4.1.4ğŸ“ Streaming de respuestas

- Muestra tokens en tiempo real con `TextIteratorStreamer`
- Refresca la interfaz cada 0.5 segundos

#### 4.1.5ğŸ’¬ GestiÃ³n de conversaciones

- Guarda, carga y elimina chats desde la barra lateral de Gradio
- Los JSON usan el alias (ej. `Llama-3.2`) en lugar del repo ID

#### 4.1.6ğŸ”Œ Descarga manual del modelo desde la interfaz

- Se ha aÃ±adido un botÃ³n **"ğŸ”Œ Descargar modelo de VRAM"** directamente en la interfaz del Chatbot.
- Permite liberar la GPU manualmente cuando terminas una conversaciÃ³n o si el modelo se queda pillado.
- TambiÃ©n se usa este botÃ³n para reiniciar el estado si se detectan errores de carga o saturaciÃ³n.

> El botÃ³n se encuentra justo debajo del selector de modelo.

---

## 5ğŸ–¼ï¸ Generador de ImÃ¡genes


Este mÃ³dulo permite generar imÃ¡genes a partir de texto (`txt2img`) usando modelos locales optimizados. 

La app utiliza Gradio como frontend y FastAPI para su integraciÃ³n en la web principal.

---

### 5.1âš™ï¸ CaracterÃ­sticas principales

#### 5.1.1ğŸ¨ Modelos soportados

Actualmente se soportan dos modelos con configuraciones distintas:

| Modelo                    | Tipo            | Resoluciones disponibles              | Optimizaciones  |
|--------------------------|------------------|----------------------------------------|-----------------|
| `realisticvision-v6`     | `sd15_safetensors` | 512x768, 768x512, 640x832, ...         | Hires.Fix, Upscaler |
| `juggernautxl`           | `sdxl_safetensors` | 1024x1024, 896x1152, 1216x832, ...     | Upscaler       |

Cada modelo se define en `images/utils/config.py` e incluye:

- `repo_id`: repositorio de Hugging Face
- `type`: tipo (`sd15_safetensors`, `sdxl_safetensors`, etc.)
- `cfg_scale`, `steps`, y resoluciones

---

#### 5.1.2ğŸ§  Flujo de generaciÃ³n

1. Se selecciona un modelo, prompt, resoluciÃ³n, y (opcionalmente) un `seed`.
2. El modelo se carga y cachea si no lo estaba.
3. Se genera la imagen base.
4. Se aplica `Hires.Fix` automÃ¡ticamente si el modelo lo permite (`type == sd15_safetensors`).
5. Si el usuario lo indica, se aplica un `Upscaler` (`realistic`, `anime`, `general`).
6. Ambas versiones se guardan y muestran al usuario (`_base.png` y `_final.png`).

---

#### 5.1.3ğŸ§ª Mejoras automÃ¡ticas

- **Hires.Fix**: mejora la nitidez y detalle aplicando `denoising_strength` y `upscale_factor` configurables.
- **Upscaler**: integra `RealESRGAN` con varios modelos para mejorar la resoluciÃ³n final.

Ambas funciones estÃ¡n desacopladas y definidas en:

- `images/utils/hires_fix.py`
- `images/utils/upscaler.py`

---

### 5.1.4ğŸ§© Interfaz visual

La UI de Gradio permite:
- Interfaz con botÃ³n para desplegar ayuda de como usar la aplicaciÃ³n
  - Visualizar ejemplos con botones de "ANTES" y "DESPUÃ‰S"
  - Las imÃ¡genes estÃ¡n generadas con la semilla 42 por si quieres comprobarlo.

- Escribir el prompt  
- Seleccionar modelo
- Elegir resoluciÃ³n recomendada (segÃºn el modelo)
- Establecer seed (opcional, usado para reproducibilidad de imÃ¡genes)
- Seleccionar upscaler (opcional)
- Ver resultados: imagen base y final
- Recargar galerÃ­a de imÃ¡genes generadas

---

### 5.1.5ğŸ–¼ï¸ GalerÃ­as automÃ¡ticas

Se listan y actualizan automÃ¡ticamente desde `/images/outputs/`:

- ImÃ¡genes base (`*_base.png`)
- ImÃ¡genes finales (`*_final.png`)

Cada generaciÃ³n aÃ±ade los nuevos archivos ordenados por fecha reciente.

---

### 5.1.6â±ï¸ Vigilancia automÃ¡tica y descarga de modelos

Cada vez que se genera una imagen, LocalHub evalÃºa el **uso actual de memoria RAM y VRAM** desde consola.  
Esto te permite ver si tu GPU estÃ¡ saturada y actuar en consecuencia (reiniciar, descargar modelos, etc.).

AdemÃ¡s, si una generaciÃ³n tarda mÃ¡s de 2 minutos, el modelo se descarga automÃ¡ticamente para evitar bloqueos.  
Este sistema actÃºa como mecanismo de seguridad para liberar la GPU sin intervenciÃ³n manual.

> âš ï¸ Este comportamiento solo es visible desde la **consola**, no desde la interfaz de usuario.

---

## 6.ğŸ¤ Transcriptor de Audio con Whisper v3

Esta app permite transcribir archivos `.mp3` o `.wav` a texto con marcas de tiempo. 

Usa modelos locales (`faster-whisper`) optimizados para ejecutarse en GPU o CPU.

### 6.1ğŸš€ CaracterÃ­sticas destacadas

- Carga modelos automÃ¡ticamente (turbo o accurate).
- DetecciÃ³n automÃ¡tica de idioma o selecciÃ³n manual.
- Salida en formato `.txt` y `.srt`.
- Muestra texto en tiempo real tras procesar el audio.
- **Incluye audios de prueba** para testear directamente.
- AdemÃ¡s, se ha aÃ±adido un botÃ³n para **descargar el modelo cargado de la VRAM** manualmente.
- Esto permite liberar memoria cuando terminas una transcripciÃ³n larga o deseas cambiar de modelo.


### 6.2ğŸ“ Audios de ejemplo

Incluidos por defecto dentro de `/spch_to_text/assets/`:

- `prueba_1_reloj.mp3`: Voz clara con palabras comunes.
- `prueba_2_silencio.mp3`: Prueba de detecciÃ³n en fragmentos vacÃ­os.
- `prueba_3_lucia.mp3`: NarraciÃ³n fluida con expresividad.

### 6.3ğŸ§  Modelos utilizados

| Modo       | Modelo                                   | VRAM recomendada |
|------------|-------------------------------------------|------------------|
| `turbo`    | `mobiuslabsgmbh/faster-whisper-large-v3-turbo` | â‰¥ 6 GB          |
| `accurate` | `Systran/faster-whisper-large-v3`        | â‰¥ 10 GB           |

Los modelos se almacenan localmente en `/spch_to_text/models`.

---

## 7.âœ¨ Extras

### 7.1 Pruebas automatizadas del Chatbot

```bash
python -m chatbot.test_models
```

- Ejecuta 5 preguntas por modelo  
- Mide tiempos de carga y generaciÃ³n  
- Registra uso de VRAM antes/despuÃ©s  
- Guarda ejemplos en: `chatbot/saved_chats/Ejemplo_<Alias>.json`

### 7.2 Ejemplos de imÃ¡genes para Generador de imÃ¡genes

```bash
python -m images.generar_ejemplos
```

- Crea 5 imÃ¡genes por modelo  
- Mide tiempos de creaciÃ³n de todos los pasos   
- Guarda ejemplos en: 
  - `images/examples/<Modelo>_<nÃºmero>_base.json`
  - `images/examples/<Modelo>_<nÃºmero>_final.json`

### 7.3 ğŸ§  AutogestiÃ³n de Recursos

- Todas las miniapps (`chatbot`, `images`, `spch_to_text`) cuentan ahora con un **sistema de timeout**.
- Si una operaciÃ³n tarda **mÃ¡s de 2 minutos**, se asume que estÃ¡ colgada y el modelo se descarga de la VRAM automÃ¡ticamente.
- Este sistema evita bloqueos persistentes y mejora la estabilidad general del sistema.
- TambiÃ©n se imprime en consola el uso de recursos antes de cada generaciÃ³n de imagen:
- AdemÃ¡s del sistema automÃ¡tico de timeout, **se han aÃ±adido botones manuales en Chatbot y Spch_to_Text** para permitir al usuario descargar el modelo activamente desde la interfaz de Gradio.

> Si ves valores muy altos, es recomendable **cerrar otras aplicaciones o descargar modelos que no estÃ©s usando**.

---

## 8.ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT.  
Â© 2025 vjimmes2003

---
